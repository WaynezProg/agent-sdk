# 07_advanced_features.py - LlamaIndex é€²éšåŠŸèƒ½
import os
from dotenv import load_dotenv
from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader,
    Settings,
    Document,
    StorageContext
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
import chromadb

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def setup_advanced_system():
    """è¨­å®šé€²éšç³»çµ±"""
    print("ğŸ”§ è¨­å®šé€²éš LlamaIndex ç³»çµ±...")
    
    # è¨­å®šåµŒå…¥æ¨¡å‹
    embed_model = OpenAIEmbedding(
        model="text-embedding-3-small",
        embed_batch_size=10
    )
    Settings.embed_model = embed_model
    
    # è¼‰å…¥æ–‡ä»¶
    documents_dir = "sample_documents"
    if not os.path.exists(documents_dir):
        print("âŒ æ‰¾ä¸åˆ°ç¯„ä¾‹æ–‡ä»¶ï¼Œè«‹å…ˆåŸ·è¡Œ 02_document_loading.py")
        return None
    
    reader = SimpleDirectoryReader(input_dir=documents_dir)
    documents = reader.load_data()
    
    print("âœ… é€²éšç³»çµ±è¨­å®šå®Œæˆï¼")
    return documents

def demonstrate_custom_node_parser(documents):
    """ç¤ºç¯„è‡ªå®šç¾©ç¯€é»è§£æå™¨"""
    print("\nâœ‚ï¸ è‡ªå®šç¾©ç¯€é»è§£æå™¨...")
    
    # å‰µå»ºè‡ªå®šç¾©åˆ†å‰²å™¨
    custom_splitter = SentenceSplitter(
        chunk_size=512,      # è¼ƒå°çš„å¡Šå¤§å°
        chunk_overlap=100,   # å¢åŠ é‡ç–Š
        separator="ã€‚",      # ä½¿ç”¨å¥è™Ÿåˆ†å‰²
        paragraph_separator="\n\n"  # æ®µè½åˆ†éš”ç¬¦
    )
    
    # å»ºç«‹ç´¢å¼•
    index = VectorStoreIndex.from_documents(
        documents,
        transformations=[custom_splitter]
    )
    
    print("âœ… è‡ªå®šç¾©ç¯€é»è§£æå™¨è¨­å®šå®Œæˆï¼")
    print(f"   å¡Šå¤§å°: {custom_splitter.chunk_size}")
    print(f"   é‡ç–Šå¤§å°: {custom_splitter.chunk_overlap}")
    print(f"   åˆ†éš”ç¬¦: {custom_splitter.separator}")
    
    return index

def demonstrate_advanced_retrievers(index):
    """ç¤ºç¯„é€²éšæª¢ç´¢å™¨"""
    print("\nğŸ¯ é€²éšæª¢ç´¢å™¨...")
    
    # å»ºç«‹å‘é‡æª¢ç´¢å™¨
    vector_retriever = VectorIndexRetriever(
        index=index,
        similarity_top_k=5
    )
    
    # æ¸¬è©¦æª¢ç´¢
    query = "äººå·¥æ™ºæ…§çš„æ‡‰ç”¨"
    nodes = vector_retriever.retrieve(query)
    
    print(f"æŸ¥è©¢: {query}")
    print(f"æª¢ç´¢åˆ° {len(nodes)} å€‹ç¯€é»")
    
    for i, node in enumerate(nodes[:3], 1):
        score = node.score if hasattr(node, 'score') else 'N/A'
        print(f"ç¯€é» {i}: åˆ†æ•¸ = {score}")
        print(f"å…§å®¹: {node.text[:100]}...")

def demonstrate_postprocessors(index):
    """ç¤ºç¯„å¾Œè™•ç†å™¨"""
    print("\nğŸ”§ å¾Œè™•ç†å™¨...")
    
    # å»ºç«‹æª¢ç´¢å™¨
    retriever = VectorIndexRetriever(
        index=index,
        similarity_top_k=10
    )
    
    # ç›¸ä¼¼åº¦éæ¿¾å™¨
    similarity_filter = SimilarityPostprocessor(
        similarity_cutoff=0.7
    )
    
    # é—œéµå­—éæ¿¾å™¨
    keyword_filter = KeywordNodePostprocessor(
        required_keywords=["äººå·¥æ™ºæ…§", "æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¸ç¿’"],
        exclude_keywords=["åˆªé™¤", "ç§»é™¤"]
    )
    
    # æ¸¬è©¦æŸ¥è©¢
    query = "æ©Ÿå™¨å­¸ç¿’çš„æ‡‰ç”¨"
    nodes = retriever.retrieve(query)
    
    print(f"åŸå§‹æª¢ç´¢çµæœ: {len(nodes)} å€‹ç¯€é»")
    
    # æ‡‰ç”¨ç›¸ä¼¼åº¦éæ¿¾
    filtered_nodes = similarity_filter.postprocess_nodes(nodes, query_bundle=None)
    print(f"ç›¸ä¼¼åº¦éæ¿¾å¾Œ: {len(filtered_nodes)} å€‹ç¯€é»")
    
    # æ‡‰ç”¨é—œéµå­—éæ¿¾
    keyword_filtered_nodes = keyword_filter.postprocess_nodes(filtered_nodes, query_bundle=None)
    print(f"é—œéµå­—éæ¿¾å¾Œ: {len(keyword_filtered_nodes)} å€‹ç¯€é»")

def demonstrate_chroma_integration(documents):
    """ç¤ºç¯„ ChromaDB æ•´åˆ"""
    print("\nğŸŒˆ ChromaDB æ•´åˆ...")
    
    try:
        # åˆå§‹åŒ– ChromaDB
        chroma_client = chromadb.PersistentClient(path="./chroma_advanced")
        collection = chroma_client.get_or_create_collection(
            name="advanced_tutorial",
            metadata={"description": "é€²éšæ•™å­¸ç¯„ä¾‹"}
        )
        
        # å»ºç«‹å‘é‡å„²å­˜
        vector_store = ChromaVectorStore(chroma_collection=collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        # å»ºç«‹ç´¢å¼•
        index = VectorStoreIndex.from_documents(
            documents,
            storage_context=storage_context
        )
        
        print("âœ… ChromaDB æ•´åˆå®Œæˆï¼")
        print(f"   é›†åˆåç¨±: {collection.name}")
        print(f"   æ–‡ä»¶æ•¸é‡: {collection.count()}")
        
        # æ¸¬è©¦æŸ¥è©¢
        query_engine = index.as_query_engine()
        response = query_engine.query("ä»€éº¼æ˜¯é›²ç«¯é‹ç®—ï¼Ÿ")
        print(f"æŸ¥è©¢çµæœ: {response.response[:100]}...")
        
        return index
        
    except Exception as e:
        print(f"âŒ ChromaDB æ•´åˆå¤±æ•—: {e}")
        return None

def demonstrate_hybrid_search(index):
    """ç¤ºç¯„æ··åˆæœå°‹"""
    print("\nğŸ”€ æ··åˆæœå°‹...")
    
    # å»ºç«‹æŸ¥è©¢å¼•æ“
    query_engine = index.as_query_engine(
        response_mode="compact",
        similarity_top_k=3
    )
    
    # æ¸¬è©¦ä¸åŒé¡å‹çš„æŸ¥è©¢
    queries = [
        "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ",  # æ¦‚å¿µæ€§æŸ¥è©¢
        "AI æ‡‰ç”¨é ˜åŸŸ",      # é—œéµå­—æŸ¥è©¢
        "æ©Ÿå™¨å­¸ç¿’ vs æ·±åº¦å­¸ç¿’"  # æ¯”è¼ƒæŸ¥è©¢
    ]
    
    for query in queries:
        print(f"\næŸ¥è©¢: {query}")
        response = query_engine.query(query)
        print(f"å›ç­”: {response.response[:150]}...")

def demonstrate_custom_embeddings():
    """ç¤ºç¯„è‡ªå®šç¾©åµŒå…¥"""
    print("\nğŸ§  è‡ªå®šç¾©åµŒå…¥...")
    
    # å‰µå»ºè‡ªå®šç¾©åµŒå…¥æ¨¡å‹
    custom_embed_model = OpenAIEmbedding(
        model="text-embedding-3-small",
        embed_batch_size=5,
        api_key=os.getenv('OPENAI_API_KEY')
    )
    
    # æ¸¬è©¦åµŒå…¥
    texts = [
        "äººå·¥æ™ºæ…§æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯",
        "æ©Ÿå™¨å­¸ç¿’æ˜¯ AI çš„é‡è¦çµ„æˆéƒ¨åˆ†",
        "æ·±åº¦å­¸ç¿’ä½¿ç”¨ç¥ç¶“ç¶²è·¯"
    ]
    
    print("æ¸¬è©¦æ–‡å­—åµŒå…¥:")
    for i, text in enumerate(texts, 1):
        embedding = custom_embed_model.get_text_embedding(text)
        print(f"æ–‡å­— {i}: {text}")
        print(f"åµŒå…¥ç¶­åº¦: {len(embedding)}")
        print(f"å‰ 5 å€‹å€¼: {embedding[:5]}")

def demonstrate_metadata_filtering(index):
    """ç¤ºç¯„å…ƒæ•¸æ“šéæ¿¾"""
    print("\nğŸ·ï¸ å…ƒæ•¸æ“šéæ¿¾...")
    
    # å»ºç«‹å¸¶æœ‰å…ƒæ•¸æ“šéæ¿¾çš„æŸ¥è©¢å¼•æ“
    query_engine = index.as_query_engine(
        filters={"file_path": "sample_ai.txt"},
        response_mode="compact"
    )
    
    query = "äººå·¥æ™ºæ…§çš„å®šç¾©"
    print(f"æŸ¥è©¢: {query}")
    print("éæ¿¾æ¢ä»¶: åªæŸ¥è©¢ sample_ai.txt æ–‡ä»¶")
    
    response = query_engine.query(query)
    print(f"å›ç­”: {response.response}")

def demonstrate_streaming_response(index):
    """ç¤ºç¯„ä¸²æµå›æ‡‰"""
    print("\nğŸŒŠ ä¸²æµå›æ‡‰...")
    
    # å»ºç«‹ä¸²æµæŸ¥è©¢å¼•æ“
    query_engine = index.as_query_engine(
        streaming=True,
        response_mode="compact"
    )
    
    query = "è«‹è©³ç´°èªªæ˜äººå·¥æ™ºæ…§çš„ç™¼å±•æ­·å²"
    print(f"æŸ¥è©¢: {query}")
    print("ä¸²æµå›æ‡‰:")
    
    # åŸ·è¡Œä¸²æµæŸ¥è©¢
    response = query_engine.query(query)
    
    # æ¨¡æ“¬ä¸²æµè¼¸å‡º
    if hasattr(response, 'response_gen'):
        for chunk in response.response_gen:
            print(chunk, end='', flush=True)
    else:
        print(response.response)

def explain_advanced_features():
    """è§£é‡‹é€²éšåŠŸèƒ½"""
    print("\nğŸ“š é€²éšåŠŸèƒ½èªªæ˜:")
    print("""
    1. âœ‚ï¸ è‡ªå®šç¾©ç¯€é»è§£æå™¨
       - æ§åˆ¶æ–‡ä»¶åˆ†å‰²ç­–ç•¥
       - å„ªåŒ–å¡Šå¤§å°å’Œé‡ç–Š
       - æ”¯æ´å¤šç¨®åˆ†å‰²æ–¹å¼
    
    2. ğŸ¯ é€²éšæª¢ç´¢å™¨
       - å¤šç¨®æª¢ç´¢ç­–ç•¥
       - è‡ªå®šç¾©ç›¸ä¼¼åº¦è¨ˆç®—
       - æ”¯æ´è¤‡é›œæŸ¥è©¢
    
    3. ğŸ”§ å¾Œè™•ç†å™¨
       - ç›¸ä¼¼åº¦éæ¿¾
       - é—œéµå­—éæ¿¾
       - è‡ªå®šç¾©éæ¿¾é‚è¼¯
    
    4. ğŸ’¾ å‘é‡è³‡æ–™åº«æ•´åˆ
       - ChromaDB
       - Pinecone
       - Weaviate
       - è‡ªå®šç¾©å‘é‡å„²å­˜
    
    5. ğŸ”€ æ··åˆæœå°‹
       - çµåˆå¤šç¨®æª¢ç´¢æ–¹å¼
       - èªç¾© + é—œéµå­—æœå°‹
       - æé«˜æª¢ç´¢æº–ç¢ºæ€§
    
    6. ğŸ§  è‡ªå®šç¾©åµŒå…¥
       - å¤šç¨®åµŒå…¥æ¨¡å‹
       - æ‰¹æ¬¡è™•ç†å„ªåŒ–
       - è‡ªå®šç¾©åµŒå…¥ç­–ç•¥
    
    7. ğŸ·ï¸ å…ƒæ•¸æ“šç®¡ç†
       - è±å¯Œçš„å…ƒæ•¸æ“šæ”¯æ´
       - éˆæ´»çš„éæ¿¾æ¢ä»¶
       - çµæ§‹åŒ–è³‡è¨Šç®¡ç†
    
    8. ğŸŒŠ ä¸²æµè™•ç†
       - å³æ™‚å›æ‡‰ç”Ÿæˆ
       - æ”¹å–„ä½¿ç”¨è€…é«”é©—
       - æ”¯æ´é•·æ–‡æœ¬ç”Ÿæˆ
    """)

def demonstrate_performance_optimization(index):
    """ç¤ºç¯„æ•ˆèƒ½å„ªåŒ–"""
    print("\nâš¡ æ•ˆèƒ½å„ªåŒ–...")
    
    import time
    
    # æ¸¬è©¦ä¸åŒé…ç½®çš„æ•ˆèƒ½
    configurations = [
        {"similarity_top_k": 3, "response_mode": "compact"},
        {"similarity_top_k": 5, "response_mode": "compact"},
        {"similarity_top_k": 3, "response_mode": "tree_summarize"}
    ]
    
    query = "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ"
    
    for i, config in enumerate(configurations, 1):
        print(f"\né…ç½® {i}: {config}")
        
        query_engine = index.as_query_engine(**config)
        
        start_time = time.time()
        response = query_engine.query(query)
        end_time = time.time()
        
        print(f"æŸ¥è©¢æ™‚é–“: {end_time - start_time:.2f} ç§’")
        print(f"å›æ‡‰é•·åº¦: {len(response.response)} å­—å…ƒ")

if __name__ == "__main__":
    try:
        print("ğŸš€ é–‹å§‹å­¸ç¿’ LlamaIndex é€²éšåŠŸèƒ½...")
        
        # è§£é‡‹é€²éšåŠŸèƒ½
        explain_advanced_features()
        
        # è¨­å®šé€²éšç³»çµ±
        documents = setup_advanced_system()
        if not documents:
            print("âŒ ç„¡æ³•è¨­å®šé€²éšç³»çµ±ï¼Œè«‹æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
            exit(1)
        
        # è‡ªå®šç¾©ç¯€é»è§£æå™¨
        index = demonstrate_custom_node_parser(documents)
        
        # é€²éšæª¢ç´¢å™¨
        demonstrate_advanced_retrievers(index)
        
        # å¾Œè™•ç†å™¨
        demonstrate_postprocessors(index)
        
        # ChromaDB æ•´åˆ
        chroma_index = demonstrate_chroma_integration(documents)
        
        # æ··åˆæœå°‹
        demonstrate_hybrid_search(index)
        
        # è‡ªå®šç¾©åµŒå…¥
        demonstrate_custom_embeddings()
        
        # å…ƒæ•¸æ“šéæ¿¾
        demonstrate_metadata_filtering(index)
        
        # ä¸²æµå›æ‡‰
        demonstrate_streaming_response(index)
        
        # æ•ˆèƒ½å„ªåŒ–
        demonstrate_performance_optimization(index)
        
        print("\nğŸ‰ LlamaIndex é€²éšåŠŸèƒ½å­¸ç¿’å®Œæˆï¼")
        print("ä¸‹ä¸€æ­¥ï¼šå­¸ç¿’ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²")
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()