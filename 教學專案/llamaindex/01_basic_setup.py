# 01_basic_setup.py - LlamaIndex åŸºç¤è¨­å®šèˆ‡æ¦‚å¿µä»‹ç´¹
import os
from dotenv import load_dotenv
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def setup_llamaindex():
    """è¨­å®š LlamaIndex çš„åŸºæœ¬é…ç½®"""
    print("ğŸš€ é–‹å§‹è¨­å®š LlamaIndex...")
    
    # æª¢æŸ¥ API Key
    api_key = os.getenv('openaikey')
    if not api_key:
        raise ValueError("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š openaikey")
    
    print(f"âœ… API Key å·²è¼‰å…¥: {api_key[:10]}...")
    
    # è¨­å®š LLM
    llm = OpenAI(
        api_key=api_key,
        model="gpt-3.5-turbo",
        temperature=0.1,
        max_tokens=512
    )
    
    # è¨­å®šåµŒå…¥æ¨¡å‹
    embed_model = OpenAIEmbedding(
        api_key=api_key,
        model="text-embedding-3-small"
    )
    
    # å…¨åŸŸè¨­å®š
    Settings.llm = llm
    Settings.embed_model = embed_model
    Settings.chunk_size = 1024
    Settings.chunk_overlap = 200
    
    print("âœ… LlamaIndex è¨­å®šå®Œæˆï¼")
    print(f"   - LLM: {llm.model}")
    print(f"   - åµŒå…¥æ¨¡å‹: {embed_model.model_name}")
    print(f"   - æ–‡ä»¶åˆ†å¡Šå¤§å°: {Settings.chunk_size}")
    print(f"   - åˆ†å¡Šé‡ç–Š: {Settings.chunk_overlap}")
    
    return llm, embed_model

def test_basic_functionality():
    """æ¸¬è©¦åŸºæœ¬åŠŸèƒ½"""
    print("\nğŸ§ª æ¸¬è©¦åŸºæœ¬åŠŸèƒ½...")
    
    # æ¸¬è©¦ LLM
    llm = Settings.llm
    response = llm.complete("è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä»€éº¼æ˜¯ RAGï¼Ÿ")
    print(f"LLM å›æ‡‰: {response.text}")
    
    # æ¸¬è©¦åµŒå…¥æ¨¡å‹
    embed_model = Settings.embed_model
    embeddings = embed_model.get_text_embedding("é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡å­—")
    print(f"åµŒå…¥å‘é‡ç¶­åº¦: {len(embeddings)}")
    print(f"åµŒå…¥å‘é‡å‰ 5 å€‹å€¼: {embeddings[:5]}")

def explain_llamaindex_concepts():
    """è§£é‡‹ LlamaIndex æ ¸å¿ƒæ¦‚å¿µ"""
    print("\nğŸ“š LlamaIndex æ ¸å¿ƒæ¦‚å¿µ:")
    print("""
    1. ğŸ“„ Document: åŸå§‹æ–‡ä»¶ï¼ˆPDFã€Wordã€ç¶²é ç­‰ï¼‰
    2. ğŸ”¤ Node: æ–‡ä»¶è¢«åˆ†å‰²å¾Œçš„å°å¡Šæ–‡å­—
    3. ğŸ§  Index: å‘é‡ç´¢å¼•ï¼Œç”¨æ–¼å¿«é€Ÿæª¢ç´¢
    4. ğŸ” Retriever: æª¢ç´¢å™¨ï¼Œæ ¹æ“šæŸ¥è©¢æ‰¾åˆ°ç›¸é—œæ–‡ä»¶
    5. ğŸ¤– Query Engine: æŸ¥è©¢å¼•æ“ï¼Œæ•´åˆæª¢ç´¢å’Œç”Ÿæˆ
    6. ğŸ“Š Response: æœ€çµ‚çš„å›ç­”çµæœ
    
    å·¥ä½œæµç¨‹ï¼š
    æ–‡ä»¶ â†’ åˆ†å‰² â†’ å‘é‡åŒ– â†’ å»ºç«‹ç´¢å¼• â†’ æŸ¥è©¢ â†’ æª¢ç´¢ â†’ ç”Ÿæˆå›ç­”
    """)

if __name__ == "__main__":
    try:
        # è¨­å®š LlamaIndex
        llm, embed_model = setup_llamaindex()
        
        # è§£é‡‹æ ¸å¿ƒæ¦‚å¿µ
        explain_llamaindex_concepts()
        
        # æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
        test_basic_functionality()
        
        print("\nğŸ‰ åŸºç¤è¨­å®šå®Œæˆï¼æ‚¨å¯ä»¥ç¹¼çºŒå­¸ç¿’ä¸‹ä¸€å€‹ç¯„ä¾‹ã€‚")
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("è«‹æª¢æŸ¥æ‚¨çš„ API Key è¨­å®šå’Œç¶²è·¯é€£ç·šã€‚")