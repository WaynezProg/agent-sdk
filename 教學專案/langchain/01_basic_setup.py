# 01_basic_setup.py - LangChain åŸºç¤è¨­å®šèˆ‡æ¦‚å¿µä»‹ç´¹
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def setup_langchain():
    """è¨­å®š LangChain çš„åŸºæœ¬é…ç½®"""
    print("ğŸš€ é–‹å§‹è¨­å®š LangChain...")
    
    # æª¢æŸ¥ API Key
    api_key = os.getenv('openaikey')
    if not api_key:
        raise ValueError("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š openaikey")
    
    print(f"âœ… API Key å·²è¼‰å…¥: {api_key[:10]}...")
    
    # è¨­å®š LLM
    llm = ChatOpenAI(
        api_key=api_key,
        model="gpt-3.5-turbo",
        temperature=0.1,
        max_tokens=512
    )
    
    # è¨­å®šåµŒå…¥æ¨¡å‹
    embeddings = OpenAIEmbeddings(
        api_key=api_key,
        model="text-embedding-3-small"
    )
    
    print("âœ… LangChain è¨­å®šå®Œæˆï¼")
    print(f"   - LLM: {llm.model_name}")
    print(f"   - åµŒå…¥æ¨¡å‹: {embeddings.model}")
    
    return llm, embeddings

def test_basic_llm(llm):
    """æ¸¬è©¦åŸºæœ¬ LLM åŠŸèƒ½"""
    print("\nğŸ§ª æ¸¬è©¦åŸºæœ¬ LLM åŠŸèƒ½...")
    
    # åŸºæœ¬å°è©±
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€å€‹å‹å–„çš„åŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
        HumanMessage(content="è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä»€éº¼æ˜¯ LangChainï¼Ÿ")
    ]
    
    response = llm.invoke(messages)
    print(f"LLM å›æ‡‰: {response.content}")
    
    # æ¸¬è©¦æµå¼å›æ‡‰
    print("\nğŸŒŠ æ¸¬è©¦æµå¼å›æ‡‰:")
    for chunk in llm.stream(messages):
        print(chunk.content, end='', flush=True)
    print()

def test_prompt_templates():
    """æ¸¬è©¦æç¤ºè©æ¨¡æ¿"""
    print("\nğŸ“ æ¸¬è©¦æç¤ºè©æ¨¡æ¿...")
    
    # å‰µå»ºæç¤ºè©æ¨¡æ¿
    template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„{role}ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
        ("human", "è«‹è§£é‡‹ä»€éº¼æ˜¯{concept}ï¼Ÿ")
    ])
    
    # æ ¼å¼åŒ–æç¤ºè©
    messages = template.format_messages(
        role="æŠ€è¡“é¡§å•",
        concept="äººå·¥æ™ºæ…§"
    )
    
    print("æ ¼å¼åŒ–å¾Œçš„æç¤ºè©:")
    for message in messages:
        print(f"  {message.type}: {message.content}")
    
    return template

def test_embeddings(embeddings):
    """æ¸¬è©¦åµŒå…¥åŠŸèƒ½"""
    print("\nğŸ§  æ¸¬è©¦åµŒå…¥åŠŸèƒ½...")
    
    # æ¸¬è©¦æ–‡å­—åµŒå…¥
    texts = [
        "äººå·¥æ™ºæ…§æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯",
        "æ©Ÿå™¨å­¸ç¿’æ˜¯ AI çš„é‡è¦çµ„æˆéƒ¨åˆ†",
        "æ·±åº¦å­¸ç¿’ä½¿ç”¨ç¥ç¶“ç¶²è·¯"
    ]
    
    print("æ¸¬è©¦æ–‡å­—åµŒå…¥:")
    for i, text in enumerate(texts, 1):
        embedding = embeddings.embed_query(text)
        print(f"æ–‡å­— {i}: {text}")
        print(f"åµŒå…¥ç¶­åº¦: {len(embedding)}")
        print(f"å‰ 5 å€‹å€¼: {embedding[:5]}")
        print()

def explain_langchain_concepts():
    """è§£é‡‹ LangChain æ ¸å¿ƒæ¦‚å¿µ"""
    print("\nğŸ“š LangChain æ ¸å¿ƒæ¦‚å¿µ:")
    print("""
    1. ğŸ§  LLM (Large Language Models)
       - å¤§å‹èªè¨€æ¨¡å‹
       - æ”¯æ´å¤šç¨®æ¨¡å‹æä¾›å•†
       - çµ±ä¸€çš„ä»‹é¢
    
    2. ğŸ“ Prompts (æç¤ºè©)
       - æ¨¡æ¿åŒ–æç¤ºè©
       - è®Šæ•¸æ›¿æ›
       - å¤šç¨®æ ¼å¼æ”¯æ´
    
    3. ğŸ”— Chains (éˆå¼è™•ç†)
       - ä¸²è¯å¤šå€‹ LLM èª¿ç”¨
       - è¤‡é›œçš„è™•ç†æµç¨‹
       - å¯çµ„åˆçš„çµ„ä»¶
    
    4. ğŸ§  Memory (è¨˜æ†¶)
       - æœƒè©±è¨˜æ†¶
       - é•·æœŸè¨˜æ†¶
       - å¤šç¨®è¨˜æ†¶é¡å‹
    
    5. ğŸ¤– Agents (æ™ºèƒ½ä»£ç†)
       - å·¥å…·ä½¿ç”¨èƒ½åŠ›
       - æ±ºç­–é‚è¼¯
       - è‡ªä¸»åŸ·è¡Œä»»å‹™
    
    6. ğŸ”§ Tools (å·¥å…·)
       - å¤–éƒ¨å·¥å…·æ•´åˆ
       - è‡ªå®šç¾©å·¥å…·
       - è±å¯Œçš„å·¥å…·ç”Ÿæ…‹
    
    7. ğŸ“Š Retrievers (æª¢ç´¢å™¨)
       - æ–‡ä»¶æª¢ç´¢
       - å‘é‡æœå°‹
       - RAG ç³»çµ±æ ¸å¿ƒ
    
    8. ğŸ—ƒï¸ Vector Stores (å‘é‡å„²å­˜)
       - å‘é‡è³‡æ–™åº«
       - ç›¸ä¼¼æ€§æœå°‹
       - æŒä¹…åŒ–å„²å­˜
    """)

def demonstrate_basic_workflow(llm, template):
    """ç¤ºç¯„åŸºæœ¬å·¥ä½œæµç¨‹"""
    print("\nğŸ”„ åŸºæœ¬å·¥ä½œæµç¨‹ç¤ºç¯„...")
    
    # ä½¿ç”¨æ¨¡æ¿å’Œ LLM
    chain = template | llm
    
    # åŸ·è¡Œéˆå¼è™•ç†
    response = chain.invoke({
        "role": "æ•™è‚²å°ˆå®¶",
        "concept": "æ©Ÿå™¨å­¸ç¿’"
    })
    
    print(f"éˆå¼è™•ç†çµæœ: {response.content}")

def explain_langchain_advantages():
    """è§£é‡‹ LangChain å„ªå‹¢"""
    print("\nâœ¨ LangChain å„ªå‹¢:")
    print("""
    1. ğŸ”§ æ¨¡çµ„åŒ–è¨­è¨ˆ
       - å¯çµ„åˆçš„çµ„ä»¶
       - éˆæ´»çš„æ¶æ§‹
       - æ˜“æ–¼æ“´å±•
    
    2. ğŸŒ è±å¯Œçš„ç”Ÿæ…‹ç³»çµ±
       - å¤šç¨® LLM æ”¯æ´
       - è±å¯Œçš„å·¥å…·åº«
       - æ´»èºçš„ç¤¾ç¾¤
    
    3. ğŸš€ ç”Ÿç”¢å°±ç·’
       - ä¼æ¥­ç´šåŠŸèƒ½
       - æ•ˆèƒ½å„ªåŒ–
       - ç›£æ§å’Œæ—¥èªŒ
    
    4. ğŸ“š å®Œæ•´çš„æ–‡æª”
       - è©³ç´°çš„æ•™ç¨‹
       - è±å¯Œçš„ç¯„ä¾‹
       - æœ€ä½³å¯¦è¸æŒ‡å—
    
    5. ğŸ”„ æŒçºŒæ›´æ–°
       - å¿«é€Ÿè¿­ä»£
       - æ–°åŠŸèƒ½æ”¯æ´
       - ç¤¾ç¾¤è²¢ç»
    """)

if __name__ == "__main__":
    try:
        print("ğŸš€ é–‹å§‹å­¸ç¿’ LangChain...")
        
        # è§£é‡‹æ ¸å¿ƒæ¦‚å¿µ
        explain_langchain_concepts()
        
        # è§£é‡‹å„ªå‹¢
        explain_langchain_advantages()
        
        # è¨­å®š LangChain
        llm, embeddings = setup_langchain()
        
        # æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
        test_basic_llm(llm)
        
        # æ¸¬è©¦æç¤ºè©æ¨¡æ¿
        template = test_prompt_templates()
        
        # æ¸¬è©¦åµŒå…¥åŠŸèƒ½
        test_embeddings(embeddings)
        
        # ç¤ºç¯„åŸºæœ¬å·¥ä½œæµç¨‹
        demonstrate_basic_workflow(llm, template)
        
        print("\nğŸ‰ LangChain åŸºç¤è¨­å®šå®Œæˆï¼")
        print("ä¸‹ä¸€æ­¥ï¼šå­¸ç¿’ LLM æ¨¡å‹ä½¿ç”¨")
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()