# 02_llm_models.py - LLM æ¨¡å‹ä½¿ç”¨
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from pydantic import BaseModel, Field

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

class PersonInfo(BaseModel):
    """äººå“¡è³‡è¨Šæ¨¡å‹"""
    name: str = Field(description="å§“å")
    age: int = Field(description="å¹´é½¡")
    occupation: str = Field(description="è·æ¥­")
    hobbies: list[str] = Field(description="æ„›å¥½åˆ—è¡¨")

def setup_llm_models():
    """è¨­å®šä¸åŒçš„ LLM æ¨¡å‹"""
    print("ğŸ”§ è¨­å®š LLM æ¨¡å‹...")
    
    api_key = os.getenv('openaikey')
    if not api_key:
        raise ValueError("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š openaikey")
    
    # ChatOpenAI (æ¨è–¦ç”¨æ–¼å°è©±)
    chat_llm = ChatOpenAI(
        api_key=api_key,
        model="gpt-3.5-turbo",
        temperature=0.1,
        max_tokens=512
    )
    
    # OpenAI (ç”¨æ–¼æ–‡æœ¬ç”Ÿæˆ)
    text_llm = OpenAI(
        api_key=api_key,
        model="gpt-3.5-turbo-instruct",
        temperature=0.1,
        max_tokens=512
    )
    
    print("âœ… LLM æ¨¡å‹è¨­å®šå®Œæˆï¼")
    print(f"   - ChatOpenAI: {chat_llm.model_name}")
    print(f"   - OpenAI: {text_llm.model_name}")
    
    return chat_llm, text_llm

def demonstrate_chat_llm(chat_llm):
    """ç¤ºç¯„ ChatOpenAI ä½¿ç”¨"""
    print("\nğŸ’¬ ChatOpenAI ç¤ºç¯„...")
    
    # åŸºæœ¬å°è©±
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€å€‹å‹å–„çš„åŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
        HumanMessage(content="è«‹ä»‹ç´¹ä¸€ä¸‹ Python ç¨‹å¼èªè¨€çš„ç‰¹é»ã€‚")
    ]
    
    response = chat_llm.invoke(messages)
    print(f"å›æ‡‰: {response.content}")
    
    # å¤šè¼ªå°è©±
    print("\nğŸ”„ å¤šè¼ªå°è©±ç¤ºç¯„:")
    conversation = [
        SystemMessage(content="ä½ æ˜¯ä¸€å€‹æŠ€è¡“é¡§å•ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
        HumanMessage(content="ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ"),
        AIMessage(content="æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒä½¿é›»è…¦èƒ½å¤ åœ¨æ²’æœ‰æ˜ç¢ºç·¨ç¨‹çš„æƒ…æ³ä¸‹å­¸ç¿’å’Œæ”¹é€²ã€‚"),
        HumanMessage(content="å®ƒæœ‰å“ªäº›ä¸»è¦é¡å‹ï¼Ÿ")
    ]
    
    response = chat_llm.invoke(conversation)
    print(f"å›æ‡‰: {response.content}")

def demonstrate_text_llm(text_llm):
    """ç¤ºç¯„ OpenAI æ–‡æœ¬ç”Ÿæˆ"""
    print("\nğŸ“ OpenAI æ–‡æœ¬ç”Ÿæˆç¤ºç¯„...")
    
    # åŸºæœ¬æ–‡æœ¬ç”Ÿæˆ
    prompt = "è«‹ç”¨ç¹é«”ä¸­æ–‡å¯«ä¸€æ®µé—œæ–¼äººå·¥æ™ºæ…§ç™¼å±•çš„çŸ­æ–‡ï¼Œä¸è¶…é100å­—ï¼š"
    
    response = text_llm.invoke(prompt)
    print(f"ç”Ÿæˆçš„æ–‡æœ¬: {response}")
    
    # å‰µæ„å¯«ä½œ
    creative_prompt = """
    è«‹å‰µä½œä¸€å€‹é—œæ–¼æ©Ÿå™¨äººèˆ‡äººé¡å‹èª¼çš„çŸ­æ•…äº‹ï¼Œè¦æ±‚ï¼š
    1. ç”¨ç¹é«”ä¸­æ–‡
    2. ä¸è¶…é200å­—
    3. åŒ…å«å°è©±
    4. æœ‰æº«æš–çš„çµå±€
    """
    
    creative_response = text_llm.invoke(creative_prompt)
    print(f"\nå‰µæ„æ•…äº‹: {creative_response}")

def demonstrate_prompt_templates(chat_llm):
    """ç¤ºç¯„æç¤ºè©æ¨¡æ¿"""
    print("\nğŸ“‹ æç¤ºè©æ¨¡æ¿ç¤ºç¯„...")
    
    # ChatPromptTemplate
    chat_template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„{role}ï¼Œå…·æœ‰è±å¯Œçš„{field}çŸ¥è­˜ã€‚"),
        ("human", "è«‹è§£é‡‹ä»€éº¼æ˜¯{concept}ï¼Œä¸¦æä¾›ä¸€å€‹å¯¦éš›æ‡‰ç”¨ä¾‹å­ã€‚")
    ])
    
    # æ ¼å¼åŒ–ä¸¦åŸ·è¡Œ
    messages = chat_template.format_messages(
        role="è³‡æ–™ç§‘å­¸å®¶",
        field="æ©Ÿå™¨å­¸ç¿’",
        concept="æ·±åº¦å­¸ç¿’"
    )
    
    response = chat_llm.invoke(messages)
    print(f"æ¨¡æ¿åŒ–å›æ‡‰: {response.content}")
    
    # PromptTemplate (ç”¨æ–¼æ–‡æœ¬ç”Ÿæˆ)
    text_template = PromptTemplate(
        input_variables=["topic", "style"],
        template="è«‹ç”¨{style}çš„é¢¨æ ¼ï¼Œå¯«ä¸€ç¯‡é—œæ–¼{topic}çš„æ–‡ç« ï¼Œä¸è¶…é150å­—ï¼š"
    )
    
    formatted_prompt = text_template.format(
        topic="é‡å­è¨ˆç®—",
        style="ç§‘æ™®"
    )
    
    print(f"\næ ¼å¼åŒ–æç¤ºè©: {formatted_prompt}")

def demonstrate_output_parsers(chat_llm):
    """ç¤ºç¯„è¼¸å‡ºè§£æå™¨"""
    print("\nğŸ” è¼¸å‡ºè§£æå™¨ç¤ºç¯„...")
    
    # å­—ä¸²è§£æå™¨
    str_parser = StrOutputParser()
    
    # JSON è§£æå™¨
    json_parser = JsonOutputParser(pydantic_object=PersonInfo)
    
    # ä½¿ç”¨å­—ä¸²è§£æå™¨
    str_template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹å‹å–„çš„åŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
        ("human", "è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä»€éº¼æ˜¯å€å¡ŠéˆæŠ€è¡“ã€‚")
    ])
    
    str_chain = str_template | chat_llm | str_parser
    str_result = str_chain.invoke({})
    print(f"å­—ä¸²è§£æçµæœ: {str_result}")
    
    # ä½¿ç”¨ JSON è§£æå™¨
    json_template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹è³‡æ–™åˆ†æå¸«ï¼Œè«‹æ ¹æ“šç”¨æˆ¶æè¿°ç”Ÿæˆçµæ§‹åŒ–è³‡è¨Šã€‚"),
        ("human", "è«‹åˆ†æä»¥ä¸‹æè¿°ä¸¦ç”Ÿæˆ JSON æ ¼å¼çš„äººå“¡è³‡è¨Šï¼š{description}"),
        ("system", "è«‹ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š{format_instructions}")
    ])
    
    json_chain = json_template | chat_llm | json_parser
    
    description = "å¼µå°æ˜ï¼Œ25æ­²ï¼Œæ˜¯ä¸€åè»Ÿé«”å·¥ç¨‹å¸«ï¼Œå–œæ­¡ç¨‹å¼è¨­è¨ˆã€é–±è®€å’Œçˆ¬å±±ã€‚"
    json_result = json_chain.invoke({
        "description": description,
        "format_instructions": json_parser.get_format_instructions()
    })
    
    print(f"\nJSON è§£æçµæœ: {json_result}")

def demonstrate_streaming(chat_llm):
    """ç¤ºç¯„ä¸²æµå›æ‡‰"""
    print("\nğŸŒŠ ä¸²æµå›æ‡‰ç¤ºç¯„...")
    
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
        HumanMessage(content="è«‹è©³ç´°è§£é‡‹ä»€éº¼æ˜¯é›²ç«¯é‹ç®—ï¼ŒåŒ…æ‹¬å…¶ç‰¹é»å’Œå„ªå‹¢ã€‚")
    ]
    
    print("ä¸²æµå›æ‡‰:")
    for chunk in chat_llm.stream(messages):
        print(chunk.content, end='', flush=True)
    print()

def demonstrate_batch_processing(chat_llm):
    """ç¤ºç¯„æ‰¹æ¬¡è™•ç†"""
    print("\nğŸ“¦ æ‰¹æ¬¡è™•ç†ç¤ºç¯„...")
    
    # æº–å‚™å¤šå€‹æç¤ºè©
    prompts = [
        "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ",
        "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
        "ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’ï¼Ÿ"
    ]
    
    # æ‰¹æ¬¡è™•ç†
    messages_list = [
        [SystemMessage(content="ä½ æ˜¯ä¸€å€‹æŠ€è¡“é¡§å•ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡æ½”å›ç­”ã€‚"),
         HumanMessage(content=prompt)]
        for prompt in prompts
    ]
    
    responses = chat_llm.batch(messages_list)
    
    print("æ‰¹æ¬¡è™•ç†çµæœ:")
    for i, response in enumerate(responses, 1):
        print(f"{i}. {response.content}")

def demonstrate_model_parameters():
    """ç¤ºç¯„æ¨¡å‹åƒæ•¸èª¿æ•´"""
    print("\nâš™ï¸ æ¨¡å‹åƒæ•¸ç¤ºç¯„...")
    
    api_key = os.getenv('openaikey')
    
    # ä¸åŒæº«åº¦è¨­å®š
    temperatures = [0.1, 0.5, 0.9]
    prompt = "è«‹å‰µä½œä¸€å€‹é—œæ–¼æœªä¾†ç§‘æŠ€çš„çŸ­æ•…äº‹ï¼š"
    
    for temp in temperatures:
        print(f"\næº«åº¦ {temp}:")
        llm = ChatOpenAI(
            api_key=api_key,
            model="gpt-3.5-turbo",
            temperature=temp,
            max_tokens=100
        )
        
        response = llm.invoke([HumanMessage(content=prompt)])
        print(f"å›æ‡‰: {response.content[:100]}...")

def explain_llm_models():
    """è§£é‡‹ LLM æ¨¡å‹é¡å‹"""
    print("\nğŸ“š LLM æ¨¡å‹é¡å‹èªªæ˜:")
    print("""
    1. ğŸ’¬ ChatOpenAI
       - å°ˆç‚ºå°è©±è¨­è¨ˆ
       - æ”¯æ´å¤šè¼ªå°è©±
       - çµæ§‹åŒ–è¨Šæ¯æ ¼å¼
       - æ¨è–¦ç”¨æ–¼èŠå¤©æ‡‰ç”¨
    
    2. ğŸ“ OpenAI
       - å°ˆç‚ºæ–‡æœ¬ç”Ÿæˆè¨­è¨ˆ
       - å–®æ¬¡æç¤ºè©è™•ç†
       - é©åˆå‰µæ„å¯«ä½œ
       - æ¨è–¦ç”¨æ–¼æ–‡æœ¬ç”Ÿæˆ
    
    3. ğŸ”§ æ¨¡å‹åƒæ•¸
       - temperature: æ§åˆ¶å‰µæ„ç¨‹åº¦ (0-1)
       - max_tokens: æœ€å¤§è¼¸å‡ºé•·åº¦
       - top_p: æ ¸æ¡æ¨£åƒæ•¸
       - frequency_penalty: é »ç‡æ‡²ç½°
    
    4. ğŸ“Š è¼¸å‡ºæ ¼å¼
       - å­—ä¸²è¼¸å‡º
       - JSON è¼¸å‡º
       - çµæ§‹åŒ–è¼¸å‡º
       - è‡ªå®šç¾©è§£æå™¨
    
    5. ğŸš€ æ•ˆèƒ½å„ªåŒ–
       - æ‰¹æ¬¡è™•ç†
       - ä¸²æµå›æ‡‰
       - å¿«å–æ©Ÿåˆ¶
       - ä¸¦è¡Œè™•ç†
    """)

if __name__ == "__main__":
    try:
        print("ğŸš€ é–‹å§‹å­¸ç¿’ LLM æ¨¡å‹ä½¿ç”¨...")
        
        # è§£é‡‹æ¨¡å‹é¡å‹
        explain_llm_models()
        
        # è¨­å®šæ¨¡å‹
        chat_llm, text_llm = setup_llm_models()
        
        # ç¤ºç¯„ ChatOpenAI
        demonstrate_chat_llm(chat_llm)
        
        # ç¤ºç¯„ OpenAI
        demonstrate_text_llm(text_llm)
        
        # ç¤ºç¯„æç¤ºè©æ¨¡æ¿
        demonstrate_prompt_templates(chat_llm)
        
        # ç¤ºç¯„è¼¸å‡ºè§£æå™¨
        demonstrate_output_parsers(chat_llm)
        
        # ç¤ºç¯„ä¸²æµå›æ‡‰
        demonstrate_streaming(chat_llm)
        
        # ç¤ºç¯„æ‰¹æ¬¡è™•ç†
        demonstrate_batch_processing(chat_llm)
        
        # ç¤ºç¯„æ¨¡å‹åƒæ•¸
        demonstrate_model_parameters()
        
        print("\nğŸ‰ LLM æ¨¡å‹å­¸ç¿’å®Œæˆï¼")
        print("ä¸‹ä¸€æ­¥ï¼šå­¸ç¿’æç¤ºè©æ¨¡æ¿")
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()