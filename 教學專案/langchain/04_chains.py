# 04_chains.py - éˆå¼è™•ç†
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain
from pydantic import BaseModel, Field

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

class AnalysisResult(BaseModel):
    """åˆ†æçµæœæ¨¡å‹"""
    topic: str = Field(description="ä¸»é¡Œ")
    summary: str = Field(description="æ‘˜è¦")
    key_points: list[str] = Field(description="é—œéµé»")
    applications: list[str] = Field(description="æ‡‰ç”¨é ˜åŸŸ")

def setup_llm():
    """è¨­å®š LLM"""
    api_key = os.getenv('openaikey')
    if not api_key:
        raise ValueError("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š openaikey")
    
    return ChatOpenAI(
        api_key=api_key,
        model="gpt-3.5-turbo",
        temperature=0.1
    )

def demonstrate_basic_chains(llm):
    """ç¤ºç¯„åŸºæœ¬éˆå¼è™•ç†"""
    print("ğŸ”— åŸºæœ¬éˆå¼è™•ç†ç¤ºç¯„...")
    
    # å‰µå»ºæç¤ºè©æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
        ("human", "è«‹è§£é‡‹ä»€éº¼æ˜¯{concept}ï¼Ÿ")
    ])
    
    # å‰µå»ºè¼¸å‡ºè§£æå™¨
    output_parser = StrOutputParser()
    
    # å‰µå»ºéˆ
    chain = prompt | llm | output_parser
    
    # åŸ·è¡Œéˆ
    result = chain.invoke({"concept": "æ©Ÿå™¨å­¸ç¿’"})
    print(f"éˆå¼è™•ç†çµæœ: {result}")

def demonstrate_llm_chain(llm):
    """ç¤ºç¯„ LLMChain"""
    print("\nğŸ“‹ LLMChain ç¤ºç¯„...")
    
    # å‰µå»ºæç¤ºè©æ¨¡æ¿
    prompt = PromptTemplate(
        input_variables=["topic", "audience"],
        template="è«‹ç‚º{audience}å¯«ä¸€ç¯‡é—œæ–¼{topic}çš„ä»‹ç´¹æ–‡ç« ï¼Œä¸è¶…é200å­—ï¼š"
    )
    
    # å‰µå»º LLMChain
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    
    # åŸ·è¡Œéˆ
    result = llm_chain.run(topic="äººå·¥æ™ºæ…§", audience="å¤§å­¸ç”Ÿ")
    print(f"LLMChain çµæœ: {result}")

def demonstrate_sequential_chains(llm):
    """ç¤ºç¯„é †åºéˆ"""
    print("\nğŸ”„ é †åºéˆç¤ºç¯„...")
    
    # ç¬¬ä¸€å€‹éˆï¼šç”Ÿæˆæ‘˜è¦
    summary_prompt = PromptTemplate(
        input_variables=["topic"],
        template="è«‹ç‚º{topic}å¯«ä¸€å€‹ç°¡çŸ­çš„æ‘˜è¦ï¼Œä¸è¶…é100å­—ï¼š"
    )
    summary_chain = LLMChain(llm=llm, prompt=summary_prompt, output_key="summary")
    
    # ç¬¬äºŒå€‹éˆï¼šç”Ÿæˆè©³ç´°èªªæ˜
    detail_prompt = PromptTemplate(
        input_variables=["summary"],
        template="åŸºæ–¼ä»¥ä¸‹æ‘˜è¦ï¼Œè«‹æä¾›æ›´è©³ç´°çš„èªªæ˜ï¼š{summary}"
    )
    detail_chain = LLMChain(llm=llm, prompt=detail_prompt, output_key="details")
    
    # å‰µå»ºé †åºéˆ
    sequential_chain = SequentialChain(
        chains=[summary_chain, detail_chain],
        input_variables=["topic"],
        output_variables=["summary", "details"]
    )
    
    # åŸ·è¡Œé †åºéˆ
    result = sequential_chain({"topic": "æ·±åº¦å­¸ç¿’"})
    print(f"æ‘˜è¦: {result['summary']}")
    print(f"è©³ç´°èªªæ˜: {result['details']}")

def demonstrate_simple_sequential_chain(llm):
    """ç¤ºç¯„ç°¡å–®é †åºéˆ"""
    print("\nğŸ”— ç°¡å–®é †åºéˆç¤ºç¯„...")
    
    # ç¬¬ä¸€å€‹éˆ
    first_prompt = PromptTemplate(
        input_variables=["topic"],
        template="è«‹ç”¨ä¸€å¥è©±è§£é‡‹ä»€éº¼æ˜¯{topic}ï¼š"
    )
    first_chain = LLMChain(llm=llm, prompt=first_prompt)
    
    # ç¬¬äºŒå€‹éˆ
    second_prompt = PromptTemplate(
        input_variables=["text"],
        template="è«‹å°‡ä»¥ä¸‹æ–‡å­—æ”¹å¯«æˆæ›´å°ˆæ¥­çš„è¡¨é”ï¼š{text}"
    )
    second_chain = LLMChain(llm=llm, prompt=second_prompt)
    
    # å‰µå»ºç°¡å–®é †åºéˆ
    simple_chain = SimpleSequentialChain(
        chains=[first_chain, second_chain],
        verbose=True
    )
    
    # åŸ·è¡Œéˆ
    result = simple_chain.run("é‡å­è¨ˆç®—")
    print(f"ç°¡å–®é †åºéˆçµæœ: {result}")

def demonstrate_custom_chains(llm):
    """ç¤ºç¯„è‡ªå®šç¾©éˆ"""
    print("\nğŸ¨ è‡ªå®šç¾©éˆç¤ºç¯„...")
    
    # å®šç¾©è‡ªå®šç¾©å‡½æ•¸
    def extract_keywords(text: str) -> str:
        """æå–é—œéµå­—"""
        return f"é—œéµå­—ï¼š{text.split()[:5]}"  # ç°¡å–®çš„é—œéµå­—æå–
    
    def format_output(text: str) -> str:
        """æ ¼å¼åŒ–è¼¸å‡º"""
        return f"ğŸ“ {text}"
    
    # å‰µå»ºè‡ªå®šç¾©éˆ
    custom_chain = (
        ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
            ("human", "è«‹ç°¡æ½”åœ°è§£é‡‹ä»€éº¼æ˜¯{concept}ï¼Ÿ")
        ])
        | llm
        | StrOutputParser()
        | RunnableLambda(extract_keywords)
        | RunnableLambda(format_output)
    )
    
    # åŸ·è¡Œè‡ªå®šç¾©éˆ
    result = custom_chain.invoke({"concept": "å€å¡Šéˆ"})
    print(f"è‡ªå®šç¾©éˆçµæœ: {result}")

def demonstrate_conditional_chains(llm):
    """ç¤ºç¯„æ¢ä»¶éˆ"""
    print("\nğŸ”€ æ¢ä»¶éˆç¤ºç¯„...")
    
    # æ¢ä»¶åˆ¤æ–·å‡½æ•¸
    def route_question(inputs):
        """æ ¹æ“šå•é¡Œé¡å‹è·¯ç”±åˆ°ä¸åŒçš„è™•ç†éˆ"""
        question = inputs["question"]
        if "ä»€éº¼æ˜¯" in question:
            return "definition"
        elif "å¦‚ä½•" in question:
            return "how_to"
        else:
            return "general"
    
    # å®šç¾©éˆ
    definition_chain = (
        ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ï¼Œè«‹æä¾›æº–ç¢ºçš„å®šç¾©ã€‚"),
            ("human", "è«‹å®šç¾©ï¼š{question}")
        ])
        | llm
        | StrOutputParser()
    )
    
    how_to_chain = (
        ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ï¼Œè«‹æä¾›è©³ç´°çš„æ­¥é©Ÿèªªæ˜ã€‚"),
            ("human", "è«‹èªªæ˜ï¼š{question}")
        ])
        | llm
        | StrOutputParser()
    )
    
    general_chain = (
        ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹å‹å–„çš„åŠ©æ‰‹ï¼Œè«‹å›ç­”å•é¡Œã€‚"),
            ("human", "{question}")
        ])
        | llm
        | StrOutputParser()
    )
    
    # å‰µå»ºæ¢ä»¶éˆ
    from langchain_core.runnables import RunnableBranch
    
    conditional_chain = RunnableBranch(
        (lambda x: route_question(x) == "definition", definition_chain),
        (lambda x: route_question(x) == "how_to", how_to_chain),
        general_chain
    )
    
    # æ¸¬è©¦ä¸åŒé¡å‹çš„å•é¡Œ
    test_questions = [
        "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
        "å¦‚ä½•é–‹å§‹å­¸ç¿’ç¨‹å¼è¨­è¨ˆï¼Ÿ",
        "ä»Šå¤©å¤©æ°£æ€éº¼æ¨£ï¼Ÿ"
    ]
    
    for question in test_questions:
        result = conditional_chain.invoke({"question": question})
        print(f"å•é¡Œ: {question}")
        print(f"å›ç­”: {result[:100]}...")
        print()

def demonstrate_parallel_chains(llm):
    """ç¤ºç¯„ä¸¦è¡Œéˆ"""
    print("\nâš¡ ä¸¦è¡Œéˆç¤ºç¯„...")
    
    # å‰µå»ºå¤šå€‹ä¸¦è¡Œéˆ
    summary_chain = (
        ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ï¼Œè«‹æä¾›ç°¡æ½”çš„æ‘˜è¦ã€‚"),
            ("human", "è«‹ç‚º{topic}å¯«ä¸€å€‹æ‘˜è¦ï¼š")
        ])
        | llm
        | StrOutputParser()
    )
    
    pros_chain = (
        ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ï¼Œè«‹åˆ—å‡ºå„ªé»ã€‚"),
            ("human", "è«‹åˆ—å‡º{topic}çš„å„ªé»ï¼š")
        ])
        | llm
        | StrOutputParser()
    )
    
    cons_chain = (
        ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ï¼Œè«‹åˆ—å‡ºç¼ºé»ã€‚"),
            ("human", "è«‹åˆ—å‡º{topic}çš„ç¼ºé»ï¼š")
        ])
        | llm
        | StrOutputParser()
    )
    
    # å‰µå»ºä¸¦è¡Œéˆ
    from langchain_core.runnables import RunnableParallel
    
    parallel_chain = RunnableParallel(
        summary=summary_chain,
        pros=pros_chain,
        cons=cons_chain
    )
    
    # åŸ·è¡Œä¸¦è¡Œéˆ
    result = parallel_chain.invoke({"topic": "äººå·¥æ™ºæ…§"})
    
    print("ä¸¦è¡Œéˆçµæœ:")
    print(f"æ‘˜è¦: {result['summary']}")
    print(f"å„ªé»: {result['pros']}")
    print(f"ç¼ºé»: {result['cons']}")

def demonstrate_json_output_chain(llm):
    """ç¤ºç¯„ JSON è¼¸å‡ºéˆ"""
    print("\nğŸ“Š JSON è¼¸å‡ºéˆç¤ºç¯„...")
    
    # å‰µå»º JSON è§£æå™¨
    json_parser = JsonOutputParser(pydantic_object=AnalysisResult)
    
    # å‰µå»º JSON è¼¸å‡ºéˆ
    json_chain = (
        ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹æŠ€è¡“åˆ†æå¸«ï¼Œè«‹åˆ†æçµ¦å®šçš„ä¸»é¡Œã€‚"),
            ("human", "è«‹åˆ†æ{topic}ï¼Œä¸¦ä»¥ JSON æ ¼å¼è¿”å›çµæœã€‚\n{format_instructions}")
        ])
        | llm
        | json_parser
    )
    
    # åŸ·è¡Œ JSON éˆ
    result = json_chain.invoke({
        "topic": "æ©Ÿå™¨å­¸ç¿’",
        "format_instructions": json_parser.get_format_instructions()
    })
    
    print("JSON è¼¸å‡ºéˆçµæœ:")
    print(f"ä¸»é¡Œ: {result.topic}")
    print(f"æ‘˜è¦: {result.summary}")
    print(f"é—œéµé»: {result.key_points}")
    print(f"æ‡‰ç”¨é ˜åŸŸ: {result.applications}")

def demonstrate_error_handling_chains(llm):
    """ç¤ºç¯„éŒ¯èª¤è™•ç†éˆ"""
    print("\nğŸ›¡ï¸ éŒ¯èª¤è™•ç†éˆç¤ºç¯„...")
    
    def safe_llm_call(inputs):
        """å®‰å…¨çš„ LLM èª¿ç”¨"""
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
                ("human", "è«‹è§£é‡‹ä»€éº¼æ˜¯{concept}ï¼Ÿ")
            ])
            chain = prompt | llm | StrOutputParser()
            return chain.invoke(inputs)
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œè™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
    
    # å‰µå»ºéŒ¯èª¤è™•ç†éˆ
    error_handling_chain = RunnableLambda(safe_llm_call)
    
    # æ¸¬è©¦æ­£å¸¸æƒ…æ³
    result1 = error_handling_chain.invoke({"concept": "äººå·¥æ™ºæ…§"})
    print(f"æ­£å¸¸çµæœ: {result1[:100]}...")
    
    # æ¸¬è©¦éŒ¯èª¤æƒ…æ³ï¼ˆä½¿ç”¨ç„¡æ•ˆçš„ LLM é…ç½®ï¼‰
    try:
        # é€™è£¡å¯ä»¥æ¨¡æ“¬éŒ¯èª¤æƒ…æ³
        result2 = error_handling_chain.invoke({"concept": "æ¸¬è©¦éŒ¯èª¤è™•ç†"})
        print(f"éŒ¯èª¤è™•ç†çµæœ: {result2}")
    except Exception as e:
        print(f"æ•ç²åˆ°éŒ¯èª¤: {e}")

def explain_chain_concepts():
    """è§£é‡‹éˆå¼è™•ç†æ¦‚å¿µ"""
    print("\nğŸ“š éˆå¼è™•ç†æ¦‚å¿µèªªæ˜:")
    print("""
    1. ğŸ”— åŸºæœ¬éˆ
       - ç·šæ€§è™•ç†æµç¨‹
       - çµ„ä»¶ä¸²è¯
       - ç°¡å–®æ˜“ç”¨
       - é©åˆåŸºæœ¬ä»»å‹™
    
    2. ğŸ“‹ LLMChain
       - å°ˆé–€çš„ LLM éˆ
       - æç¤ºè© + LLM
       - è¼¸å‡ºè§£æ
       - æ¨™æº–åŒ–æµç¨‹
    
    3. ğŸ”„ é †åºéˆ
       - å¤šæ­¥é©Ÿè™•ç†
       - å‰ä¸€æ­¥è¼¸å‡ºä½œç‚ºä¸‹ä¸€æ­¥è¼¸å…¥
       - è¤‡é›œé‚è¼¯è™•ç†
       - é©åˆå¤šéšæ®µä»»å‹™
    
    4. ğŸ¨ è‡ªå®šç¾©éˆ
       - éˆæ´»çš„çµ„ä»¶çµ„åˆ
       - è‡ªå®šç¾©å‡½æ•¸
       - è¤‡é›œé‚è¼¯
       - é«˜åº¦å¯å®šåˆ¶
    
    5. ğŸ”€ æ¢ä»¶éˆ
       - æ ¹æ“šæ¢ä»¶é¸æ“‡ä¸åŒè·¯å¾‘
       - å‹•æ…‹è·¯ç”±
       - æ™ºèƒ½æ±ºç­–
       - é©åˆè¤‡é›œå ´æ™¯
    
    6. âš¡ ä¸¦è¡Œéˆ
       - åŒæ™‚åŸ·è¡Œå¤šå€‹éˆ
       - æé«˜æ•ˆç‡
       - çµæœåˆä½µ
       - é©åˆç¨ç«‹ä»»å‹™
    
    7. ğŸ“Š çµæ§‹åŒ–è¼¸å‡º
       - JSON æ ¼å¼è¼¸å‡º
       - é¡å‹å®‰å…¨
       - æ˜“æ–¼è™•ç†
       - é©åˆ API æ•´åˆ
    
    8. ğŸ›¡ï¸ éŒ¯èª¤è™•ç†
       - ç•°å¸¸æ•ç²
       - å„ªé›…é™ç´š
       - æé«˜ç©©å®šæ€§
       - ç”Ÿç”¢ç’°å¢ƒå¿…å‚™
    """)

if __name__ == "__main__":
    try:
        print("ğŸš€ é–‹å§‹å­¸ç¿’éˆå¼è™•ç†...")
        
        # è§£é‡‹éˆå¼è™•ç†æ¦‚å¿µ
        explain_chain_concepts()
        
        # è¨­å®š LLM
        llm = setup_llm()
        
        # åŸºæœ¬éˆ
        demonstrate_basic_chains(llm)
        
        # LLMChain
        demonstrate_llm_chain(llm)
        
        # é †åºéˆ
        demonstrate_sequential_chains(llm)
        
        # ç°¡å–®é †åºéˆ
        demonstrate_simple_sequential_chain(llm)
        
        # è‡ªå®šç¾©éˆ
        demonstrate_custom_chains(llm)
        
        # æ¢ä»¶éˆ
        demonstrate_conditional_chains(llm)
        
        # ä¸¦è¡Œéˆ
        demonstrate_parallel_chains(llm)
        
        # JSON è¼¸å‡ºéˆ
        demonstrate_json_output_chain(llm)
        
        # éŒ¯èª¤è™•ç†éˆ
        demonstrate_error_handling_chains(llm)
        
        print("\nğŸ‰ éˆå¼è™•ç†å­¸ç¿’å®Œæˆï¼")
        print("ä¸‹ä¸€æ­¥ï¼šå­¸ç¿’è¨˜æ†¶æ©Ÿåˆ¶")
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()